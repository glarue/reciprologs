#!/usr/bin/env python3  
# the above sources Python from $PATH
##!/usr/local/bin/python3
##!/usr/bin/python3
# the above uses specific Python version; allows script name in top


# authorship information
__author__ = 'Graham E. Larue'
__maintainer__ = "Graham E. Larue"
__email__ = 'egrahamlarue@gmail.com'
__license__ = 'GPL'

"""
usage: reciprologs [-h] [-p [PARALLEL_PROCESSES]] [-t PERCENTAGE]
                   [--overwrite] [--one_to_one] [--logging]
                   file_1 file_2 ... [file_1 file_2 ... ...]
                   {diamondp,diamondx,blastn,blastp,blastx,tblastn,tblastx}

Find reciprocal best hits between two or more files.

positional arguments:
  file_1 file_2 ...     files to use to build reciprolog sets (space
                        separated)
  {diamondp,diamondx,blastn,blastp,blastx,tblastn,tblastx}
                        type of alignment program to run

optional arguments:
  -h, --help            show this help message and exit
  -p [PARALLEL_PROCESSES], --parallel_processes [PARALLEL_PROCESSES]
                        run the alignment step using multiple parallel
                        processes; without specific argument, will use half
                        available system CPUs (default: None)
  -t PERCENTAGE, --query_percentage_threshold PERCENTAGE
                        require a specified fraction of the query length to
                        match in order for a hit to qualify (lowest allowable
                        percentage (default: None)
  --overwrite           overwrite existing output files (instead of using to
                        bypass alignment) (default: False)
  --one_to_one          remove any many-to-one reciprolog relationships in
                        each pairwise set, such that each member of each
                        pairwise comparison is only present exactly one time
                        in output (default: False)
  --logging             output a log of best-hit choice criteria (default:
                        False)

NOTE: Depends on palign

"""
import sys
import subprocess
import os
import time
import argparse
import re
from operator import itemgetter
from multiprocessing import cpu_count
from collections import defaultdict
from itertools import combinations, permutations
from biogl import fasta_parse, get_runtime

# use networkx library for fast ortholog clustering if available
try:
    import networkx as nx
    USE_GRAPH = True
except ModuleNotFoundError:
    USE_GRAPH = False

def parse_blast_line(bl, *args):
    """
    Returns info from certain columns in a tab-separated BLAST
    output file. $args may be: query, subject, length, e, bitscore

    """
    columns = bl.strip().split("\t")
    (query, subject, length, 
    e_value, bitscore, q_start,
    q_stop) = itemgetter(0, 1, 3, 10, 11, 6, 7)(columns)
    arg_map = {
        "query": query,
        "subject": subject,
        "length": int(length) - 1,  # seem to be off by 1 in BLAST output
        "e": float(e_value),
        "bitscore": float(bitscore)
    }
    results = []
    for a in args:
        results.append(arg_map[a])
    if len(results) == 1:
        results = results[0]
    return results


def is_better(challenger, defender, seq_lengths=None):
    """
    Compares attributes of two dictionaries of BLAST
    hits for a given query to determine which is better.

    Returns the winning dictionary and reason if it's 
    better, otherwise False.
    
    """
    cbs = challenger['score']
    dbs = defender['score']
    # criteria: bitscore
    if cbs < dbs:
        return False
    elif cbs > dbs:
        return challenger, 'bitscore'
    elif cbs == dbs:
        # criteria --> e-value
        cev = challenger['evalue']
        dev = defender['evalue']
        if cev < dev: # lower is better
            return challenger, 'e-value'
        elif seq_lengths is not None:
            # criteria --> length
            # if scores are equal, check if sequence lengths
            # have been provided as an additional tiebreaking
            # criteria and look up the subject length to
            # see if there's a difference
            dn = defender['name']
            cn = challenger['name']
            try:
                if seq_lengths[cn] > seq_lengths[dn]:
                    return challenger, 'length'
            except KeyError:
                return False
        else:
            return False
    else:
        return False


def get_prefix(seq_id, delimiter):
    split_list = re.split(delimiter, seq_id, maxsplit=1)
    split_list = [s for s in split_list if s]

    return split_list[0]


def get_top_hits(
    blast, 
    paralogs=False, 
    query_match=None, 
    seq_lengths=None,
    ignore_same_id=False,
    ignore_same_prefix=False,
    query_list=None):
    results = {}
    # dictionary to store tie-broken matches
    win_ledger = defaultdict(lambda: defaultdict(set))
    with open(blast) as blst:
        for l in blst:
            new_best_hit = False
            if l.startswith("#"):
                continue
            (q, s, score, length, evalue) = parse_blast_line(
                l, "query", "subject", "bitscore", "length", "e")
            challenger = {
                'name': s,
                'score': score,
                'evalue': evalue
            }
            if query_list and q not in query_list:
                continue

            # do not consider hits to self if BLASTing against self,
            # but allow query/subject names to be the same
            if q == s and (paralogs is True or ignore_same_id is True):
            # if paralogs and q == s:
                continue
            if ignore_same_prefix is not None:
                prefix = ignore_same_prefix
                if get_prefix(q, prefix) == get_prefix(s, prefix):
                    continue
            if query_match:
                # use query_match dictionary to compare query lengths to
                # match lengths to exclude matches where query percentage 
                # is below query_match_threshold key
                fraction = (length / query_match[q]) * 100
                if fraction < query_match['query_match_threshold']:
                    continue
            if q in results:
                defender = results[q]
                challenger_wins = is_better(
                    challenger, defender, seq_lengths)
                if challenger_wins:  # new hit is better
                    new_best_hit = True
                    defender_name = results[q]['name']
                    reason = challenger_wins[1]
                    loser_info = (defender_name, reason)
                    win_ledger[q]['losers'].add(loser_info)
                    win_ledger[q]['best'] = s
                    
            else:
                new_best_hit = True

            if new_best_hit is True:
                results[q] = {"name": s, "score": score, "evalue": evalue}

    return results, win_ledger


def get_reciprocals(d1, d2):
    """
    Takes two dictionaries of top BLAST hits,
    returns a list of tuples of all pairs that were
    reciprocal best hits, along with their bitscore
    values.

    """
    reciprologs = set()
    blast_combos = [(d1, d2), (d2, d1)]
    for first, second in blast_combos:
        for query, hit_info in first.items():
            best_hit = hit_info["name"]
            score = hit_info["score"]
            if best_hit in second:
                reciprocal_hit = second[best_hit]["name"]
                if query == reciprocal_hit:  # best hit refers back to query
                    r_score = second[best_hit]["score"]
                    hit_pair = sorted([query, best_hit])
                    score_tuple = tuple(sorted([score, r_score]))
                    hit_pair.append(score_tuple)
                    reciprologs.add(tuple(hit_pair))
    return sorted(reciprologs)


def abbreviate(name, delimiter="."):
    name = os.path.basename(name)  # in case of non-local file path
    abbreviation = name.split(delimiter)[0]
    return abbreviation


def unique_filenames(*file_list):
    abbreviated = [abbreviate(f) for f in file_list]
    if len(set(abbreviated)) < len(abbreviated):  # not all are unique
        return [os.path.basename(f) for f in file_list]
    else:
        return abbreviated


def concatenate(outname, file_list, clean=True):
    with open(outname, 'w') as outfile:
        for fn in file_list:
            with open(fn) as f:
                for l in f:
                    outfile.write(l)
    if clean:
        [os.remove(fn) for fn in file_list]


def parse_run_type(align_type_arg):
    type_map = {
        'diamondp': ('diamond', 'blastp'),
        'diamondx': ('diamond', 'blastx'),
        'blastn': ('blast', 'blastn'),
        'blastp': ('blast', 'blastp'),
        'blastx': ('blast', 'blastx'),
        'tblastn': ('blast', 'tblastn'),
        'tblastx': ('blast', 'tblastx'),
    }

    return type_map[align_type_arg]


def aggregate_dict_chained(ortho_dict):
    """
    IN:
    defaultdict(set,
            {'a': {'b', 'c', 'd'},
             'b': {'a', 'c', 'e', 'f'},
             'c': {'a', 'b', 'e', 'f', 'g'},
             'd': {'a'},
             'e': {'b', 'c'},
             'f': {'b', 'c'},
             'g': {'c'},
             'w': {'z'},
             'x': {'z'},
             'y': {'z'},
             'z': {'w', 'x', 'y'}})
    OUT:
    defaultdict(set, {'a': {'b', 'c', 'd', 'e', 'f', 'g'}, 'z': {'w', 'x', 'y'}})
    
    """
    changed = False
    processed = []
    master = defaultdict(set)
    for k, v in ortho_dict.items():
        if k in processed:
            continue
        processed.append(k)
        for v2 in v:
            if v2 == k:
                continue
            master[k].add(v2)
            processed.append(v2)
            if v2 not in ortho_dict:
                continue
            changed = True
            master[k].update(ortho_dict[v2])
    if changed is True:
        master = aggregate_dict_chained(master)

    return master


def aggregate_orthos_chained(orthos):
    """
    IN:
    [
        [('a', 'b'), ('a', 'c'), ('a', 'd')],
        [('b', 'c'), ('b', 'e'), ('b', 'f')],
        [('c', 'e'), ('c', 'f'), ('c', 'g')],
        [('z', 'x'), ('z', 'y'), ('z', 'w')]
    ]
    OUT:
    [['a', 'b', 'c', 'd', 'e', 'f', 'g'], ['w', 'x', 'y', 'z']]
    
    """
    o_dict = make_ortho_dict(*orthos)
    aggregated = aggregate_dict_chained(o_dict)
    ortho_groups = []
    for k, v in aggregated.items():
        combined = tuple(v) + (k,)
        ortho_groups.append(sorted(combined))
    
    return sorted(ortho_groups)


def aggregate_orthos_strict(orthos, use_graph=False):
    """
    IN:
    [
        [('a', 'b'), ('a', 'c'), ('a', 'd')],
        [('b', 'c'), ('b', 'e'), ('b', 'f')],
        [('c', 'e'), ('c', 'f'), ('c', 'g')],
        [('z', 'x'), ('z', 'y'), ('z', 'w')]
    ]
    OUT:
    [
        ('a', 'b', 'c'), 
        ('a', 'd'), 
        ('b', 'c', 'e'), 
        ('b', 'c', 'f'), 
        ('c', 'g'), 
        ('w', 'z'), 
        ('x', 'z'), 
        ('y', 'z')
    ]
    
    """
    if use_graph is True:
        aggregated = graph_cluster(orthos)
    else:
        o_dict = make_ortho_dict(*orthos)
        aggregated = all_by_all_orthos(o_dict)

    return aggregated


def all_by_all_orthos(ortho_dict):
    full_groups = []
    for k, v in ortho_dict.items():
        groups = []
        max_n = len(v)
        # go backward in size and cull subsets as we go
        for i in range(max_n, 0, -1):
            for g in combinations(v, i):
                g = set(list(g) + [k])
                if g in full_groups or len(g) == 1:
                    continue
                if every_member_match(g, ortho_dict):
                    if any(og.issuperset(g) for og in full_groups):
                        continue
                    full_groups.append(g)

    return sorted([sorted(g) for g in full_groups])


def every_member_match(members, m_dict):
    all_match = True
    for m in members:
        others = [e for e in members if e != m]
        if not others:
            return True
        if any(m not in m_dict[o] for o in others):
            return False
            
    return all_match


def make_ortho_dict(*orthos):
    """
    IN:
    [
        [('a', 'b'), ('a', 'c'), ('a', 'd')],
        [('b', 'c'), ('b', 'e'), ('b', 'f')],
        [('c', 'e'), ('c', 'f'), ('c', 'g')],
        [('z', 'x'), ('z', 'y'), ('z', 'w')]
    ]
    OUT:
    defaultdict(set,
            {'a': {'b', 'c', 'd'},
             'b': {'a', 'c', 'e', 'f'},
             'c': {'a', 'b', 'e', 'f', 'g'},
             'd': {'a'},
             'e': {'b', 'c'},
             'f': {'b', 'c'},
             'g': {'c'},
             'w': {'z'},
             'x': {'z'},
             'y': {'z'},
             'z': {'w', 'x', 'y'}})
    
    """
    collector = defaultdict(set)
    for o_list in orthos:
        for pair in o_list:
            for a, b in permutations(pair, 2):
                collector[a].add(b)

    return collector


def names_from_blastfile(blast_fn):
    file_pattern = r'(.+)-vs-(.+)\.t?blast[npx]'
    query_fn, subject_fn = re.findall(file_pattern, blast_fn)[0]

    return query_fn, subject_fn


def pair_reciprologs(
    query, 
    subject, 
    run_type, 
    qp, 
    extra, 
    agg_blast=None, 
    file_dirs=None):
    # special case of BLASTing against self
    PARALOGS = query == subject

    q_lengths, s_lengths = {}, {}
    # we need to get sequence lengths for each file
    # for tie-breaking by length and qp
    for fa, target in zip([query, subject], [q_lengths, s_lengths]):
        target['query_match_threshold'] = qp
        for h, s in fasta_parse(fa):
            target[h] = len(s)

    # get sets of query and subject IDs to use in filtering
    # BLAST lines to only relevant hits
    q_list = set(q_lengths.keys())
    s_list = set(s_lengths.keys())    

    # dictionaries are used as flag in top hits function
    # so need to be set here
    if not qp:
        qm_q, qm_s = {}, {}
    else:
        qm_q = q_lengths
        qm_s = s_lengths

    run_files = {
        'forward': None,
        'reverse': None
    } 

    if agg_blast is not None:
        run_files['forward'] = agg_blast
        run_files['reverse'] = agg_blast
        # fw_fn = agg_blast
        # rv_fn = agg_blast
    else:
        # align in both directions (unless PARALOGS)
        fw_names = unique_filenames(query, subject)
        rv_names = fw_names[::-1]

        run_files['forward'] = '{}-vs-{}.{}'.format(*fw_names, run_type)
        run_files['reverse'] = '{}-vs-{}.{}'.format(*rv_names, run_type)
        # fw_fn = '{}-vs-{}.{}'.format(*fw_names, run_type)
        # rv_fn = '{}-vs-{}.{}'.format(*rv_names, run_type)

    if file_dirs:
        for d in file_dirs:
            file_list = os.listdir(d)
            for direction, fn in run_files.items():
                if fn in file_list:
                    run_files[direction] = os.path.join(
                        os.path.abspath(d), fn
                    )

    # use existing BLAST output unless --overwrite is specified
    fw_fn = run_files['forward']
    rv_fn = run_files['reverse']

    if not os.path.isfile(fw_fn) or OVERWRITE:
        forward_args = [
            ALIGN_PROG, 
            query, 
            subject, 
            run_type,
            '-o', 
            fw_fn] + extra
        subprocess.run(forward_args)
    else:
        print('[#] Using existing output \'{}\''.format(fw_fn))
    top_forward, fw_win_ledger = get_top_hits(
        fw_fn, 
        PARALOGS, 
        query_match=qm_q, 
        seq_lengths=s_lengths,
        ignore_same_id=IGNORE_SAME_ID,
        ignore_same_prefix=IGNORE_SAME_PREFIX,
        query_list=q_list)
    if PARALOGS:
        top_reverse = top_forward
        rv_win_ledger = defaultdict(list)
        ## don't need to run BLAST again; process the same file
        # top_reverse, rv_win_ledger = get_top_hits(
        #     fw_fn, PARALOGS, query_match=qm_q)
    else:
        if not os.path.isfile(rv_fn) or OVERWRITE:
            reverse_args = [
                ALIGN_PROG, 
                subject, 
                query, 
                run_type, 
                '-o', 
                rv_fn] + extra
            subprocess.run(reverse_args)
        else:
            print('[#] Using existing output \'{}\''.format(rv_fn))
        top_reverse, rv_win_ledger = get_top_hits(
            rv_fn, 
            query_match=qm_s, 
            seq_lengths=q_lengths,
            ignore_same_id=IGNORE_SAME_ID,
            ignore_same_prefix=IGNORE_SAME_PREFIX,
            query_list=s_list)

    # Filter for reciprocal best hits
    reciprocal_hits = get_reciprocals(top_forward, top_reverse)

    # combine winners ledger

    win_ledger = {**fw_win_ledger, **rv_win_ledger}

    return reciprocal_hits, win_ledger


def remove_many_to_one(pairs):
    """
    Each element of >pairs< is a tuple: (hitA, hitB, (scoreX, scoreY))

    Takes a list of paired reciprocal hits (plus scores) and filters it
    such that each member of each pair only occur once, i.e. it removes
    any many-to-one hits, using the bitscores in the last element of
    each tuple.
    
    """
    uniques = {}
    to_remove = []
    for index, (a, b, scores) in enumerate(pairs):
        avg_score = sum(scores) / 2
        for e in [a, b]:
            if e not in uniques:
                uniques[e] = {'score': avg_score, 'index': index}
            elif uniques[e]['score'] >= avg_score:
                to_remove.append(index)
                continue
            else:
                to_remove.append(uniques[e]['index'])
                uniques[e] = {'score': avg_score, 'index': index}

    filtered = []
    for i, p in enumerate(pairs):
        if i in to_remove:
            names = p[0:2]
            print('Removed: {}'.format('\t'.join(names)), file=sys.stderr)
        else:
            filtered.append(p)

    return filtered


def graph_cluster(pairwise_sets):
    graph = nx.Graph()
    for p_set in pairwise_sets:
        for pair in p_set:
            graph.add_edge(*pair)
    
    clusters = nx.find_cliques(graph)
    clusters = [sorted(c) for c in clusters]

    return sorted(clusters, key=len)


parser = argparse.ArgumentParser(
    description=(
        'Find reciprocal best hits between two or more files.'),
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument(
    'input_files',
    metavar='file_1 file_2 ...',
    help='files to use to build reciprolog sets (space separated)',
    nargs='+'
)
parser.add_argument(
    'run_type',
    choices=[
        'diamondp',
        'diamondx',
        'blastn',
        'blastp',
        'blastx',
        'tblastn',
        'tblastx'],
    help='type of alignment program to run'
)
parser.add_argument(
    '-p',
    '--parallel_processes',
    help=(
        'run the alignment step using multiple parallel processes'),
    type=int,
    default=1
)
parser.add_argument(
    '-q',
    '--query_percentage_threshold',
    metavar='PERCENTAGE',
    help=(
        'require a specified fraction of the query length to match in '
        'order for a hit to qualify (lowest allowable percentage'),
    type=float,
    default=None
)
parser.add_argument(
    '--chain',
    action='store_true',
    help=(
        'cluster reciprologs without requiring all-by-all pairwise '
        'relationships, e.g. A-B, A-C, A-D --> A-B-C-D')
)
parser.add_argument(
    '--ignore_same_id',
    action='store_true',
    help='ignore hits where both query and subject have identical IDs'
)
parser.add_argument(
    '--ignore_same_prefix',
    metavar='<prefix_delimiter>',
    help=(
        'ignore hits where both query and subject have identical prefixes, '
        'where the prefix for each ID is delimited by the specified '
        '<prefix_delimiter>')
)
parser.add_argument(
    '-o',
    '--output',
    help='output filename (if no argument is given, defaults to stdout)',
    nargs='?',
    const='stdout'
)
parser.add_argument(
    '-d',
    '--alignment_source_directory',
    help='check for existing alignment files to use in this directory first',
    nargs='+'
)
parser.add_argument(
    '-b',
    '--blast_file',
    help='aggregated BLAST output to use (both directions)'
)
parser.add_argument(
    '--overwrite',
    help='overwrite existing output files (instead of using to bypass alignment)',
    action='store_true'
)
parser.add_argument(
    '--one_to_one',
    help=(
        'remove any many-to-one reciprolog relationships in each pairwise '
        'set, such that each member of each pairwise comparison is only '
        'present exactly one time in output'),
    action='store_true'
)
parser.add_argument(
    '--logging',
    help='output a log of best-hit choice criteria',
    action='store_true'
)

if len(sys.argv) == 1:
    sys.exit(parser.print_help())

t_start = time.time()

args, EXTRA_ARGS = parser.parse_known_args()

RUN_TYPE = args.run_type
PARALLEL = args.parallel_processes
INPUT_FILES = args.input_files
if len(INPUT_FILES) < 2:
    sys.exit('error: too few files specified (need >1)')
QUERY_PERCENTAGE = args.query_percentage_threshold
OVERWRITE = args.overwrite
ONE_TO_ONE = args.one_to_one
LOGGING = args.logging
CHAIN = args.chain
IGNORE_SAME_ID = args.ignore_same_id
IGNORE_SAME_PREFIX = args.ignore_same_prefix
BLAST_FILE = args.blast_file
OUTPUT_FILE = args.output
ALIGNMENT_SOURCE_DIRS = args.alignment_source_directory

ALIGN_PROG = 'palign'

if not USE_GRAPH:
    print(
        '[!] networkx library not found; will use brute-force method instead',
        file=sys.stderr
    )

# create a list with the flags/options to pass to the subsequence
# subprocess calls
call_options = {
    '-p': PARALLEL
}
optional = EXTRA_ARGS
for k, v in call_options.items():
    if v:
        optional.extend([str(k), str(v)])

# get reciprologs for each pairwise combo of files
pairwise_reciprolog_sets = []
for q, s in combinations(sorted(INPUT_FILES), 2):
    reciprolog_set, win_ledger = pair_reciprologs(
        q, 
        s, 
        RUN_TYPE, 
        QUERY_PERCENTAGE, 
        optional, 
        agg_blast=BLAST_FILE,
        file_dirs=ALIGNMENT_SOURCE_DIRS)
    if LOGGING:
        ledger_file = '{}-{}.{}.log'.format(
            abbreviate(q), abbreviate(s), args.run_type)
        with open(ledger_file, 'w') as lf:
            for query, info in sorted(win_ledger.items()):
                winner = info['best']
                loser_tuples = info['losers']
                lf.write('>{}\t[{}]\n'.format(winner, query))
                for loser in sorted(loser_tuples):
                    lf.write('\t'.join(loser) + '\n')
    if ONE_TO_ONE is True:
        reciprolog_set = remove_many_to_one(reciprolog_set)
    # remove score info
    reciprolog_set = [tuple(x[0:2]) for x in reciprolog_set]

    pairwise_reciprolog_sets.append(reciprolog_set)

if CHAIN:
    reciprologs = aggregate_orthos_chained(pairwise_reciprolog_sets)
else:
    reciprologs = aggregate_orthos_strict(pairwise_reciprolog_sets, USE_GRAPH)

basename = '-'.join([abbreviate(f) for f in INPUT_FILES])

if OUTPUT_FILE:
    if OUTPUT_FILE == 'stdout':
        out = sys.stdout
        out_string = ''
    else:
        out = open(OUTPUT_FILE, 'w')
        out_string = ': {}'.format(OUTPUT_FILE)
else:
    OUTPUT_FILE = '{}.{}.reciprologs'.format(basename, RUN_TYPE)
    out = open(OUTPUT_FILE, 'w')
    out_string = ': {}'.format(OUTPUT_FILE)

for group in reciprologs:
    out.write('\t'.join(group) + '\n')

if OUTPUT_FILE != 'stdout':
    out.close()

runtime = get_runtime(t_start)

print(
    '[#] Job finished in {}; {} reciprolog sets found{}'
    .format(runtime, len(reciprologs), out_string),
    file=sys.stderr)

sys.exit(0)
